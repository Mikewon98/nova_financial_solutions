{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a25d0a7",
   "metadata": {},
   "source": [
    "# Correlation between news and stock movement\n",
    "## ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd0283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/vader_lexicon')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NewsStockCorrelationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.stock_data = {}\n",
    "        self.news_data = None\n",
    "        self.merged_data = {}\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.correlation_results = {}\n",
    "    \n",
    "    def load_stock_data(self, file_paths):\n",
    "\n",
    "        \n",
    "        for symbol, file_path in file_paths.items():\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Date'] = pd.to_datetime(df['Date'])\n",
    "                df = df.sort_values('Date')\n",
    "                \n",
    "                # Calculate daily returns (percentage change)\n",
    "                df['Daily_Return'] = df['Close'].pct_change() * 100\n",
    "                df['Price_Change'] = df['Close'].diff()\n",
    "                df['Volatility'] = df['Daily_Return'].rolling(window=5).std()\n",
    "                \n",
    "                # Remove NaN values\n",
    "                df = df.dropna()\n",
    "                \n",
    "                self.stock_data[symbol] = df\n",
    "                print(f\"✓ {symbol}: {len(df)} records from {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error loading {symbol}: {str(e)}\")\n",
    "    \n",
    "    def load_news_data(self, file_path):\n",
    "        \"\"\"Load news data from CSV file\"\"\"\n",
    "        print(\"\\nLoading News Data...\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        try:\n",
    "            self.news_data = pd.read_csv(file_path)\n",
    "\n",
    "            # Debug: Check the date column format\n",
    "            print(\"Debug - Date column info:\")\n",
    "            if 'date' in self.news_data.columns:\n",
    "                print(f\"Date column dtype: {self.news_data['date'].dtype}\")\n",
    "                print(f\"Sample dates: {self.news_data['date'].head()}\")\n",
    "\n",
    "                # Try to convert dates immediately with error handling\n",
    "                try:\n",
    "                    self.news_data['date'] = pd.to_datetime(self.news_data['date'], errors='coerce')\n",
    "                    print(\"✓ Date conversion successful\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Date conversion issue: {str(e)}\")\n",
    "\n",
    "            # Rest of your existing code...\n",
    "            self.news_data['Date'] = pd.to_datetime(self.news_data['date']).dt.date\n",
    "            self.news_data['Date'] = pd.to_datetime(self.news_data['Date'])\n",
    "\n",
    "            print(f\" Loaded {len(self.news_data)} news articles\")\n",
    "            print(f\"  Date range: {self.news_data['Date'].min().strftime('%Y-%m-%d')} to {self.news_data['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Unique stocks: {self.news_data['stock'].nunique()}\")\n",
    "            print(f\"  Stock symbols: {list(self.news_data['stock'].unique())}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading news data: {str(e)}\")\n",
    "    \n",
    "    def perform_sentiment_analysis(self):\n",
    "        \"\"\"Perform sentiment analysis on news headlines\"\"\"\n",
    "        print(\"\\nPerforming Sentiment Analysis...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if self.news_data is None:\n",
    "            print(\"No news data loaded!\")\n",
    "            return\n",
    "        \n",
    "        # Initialize sentiment columns\n",
    "        self.news_data['textblob_polarity'] = 0.0\n",
    "        self.news_data['textblob_subjectivity'] = 0.0\n",
    "        self.news_data['vader_compound'] = 0.0\n",
    "        self.news_data['vader_positive'] = 0.0\n",
    "        self.news_data['vader_negative'] = 0.0\n",
    "        self.news_data['vader_neutral'] = 0.0\n",
    "        \n",
    "        for idx, row in self.news_data.iterrows():\n",
    "            headline = str(row['headline'])\n",
    "            \n",
    "            # TextBlob sentiment\n",
    "            blob = TextBlob(headline)\n",
    "            self.news_data.at[idx, 'textblob_polarity'] = blob.sentiment.polarity\n",
    "            self.news_data.at[idx, 'textblob_subjectivity'] = blob.sentiment.subjectivity\n",
    "            \n",
    "            # VADER sentiment\n",
    "            vader_scores = self.sia.polarity_scores(headline)\n",
    "            self.news_data.at[idx, 'vader_compound'] = vader_scores['compound']\n",
    "            self.news_data.at[idx, 'vader_positive'] = vader_scores['pos']\n",
    "            self.news_data.at[idx, 'vader_negative'] = vader_scores['neg']\n",
    "            self.news_data.at[idx, 'vader_neutral'] = vader_scores['neu']\n",
    "        \n",
    "        # Create sentiment categories\n",
    "        self.news_data['sentiment_category'] = self.news_data['textblob_polarity'].apply(\n",
    "            lambda x: 'Positive' if x > 0.1 else ('Negative' if x < -0.1 else 'Neutral')\n",
    "        )\n",
    "        \n",
    "        print(f\"Sentiment analysis completed for {len(self.news_data)} headlines\")\n",
    "        print(\"\\nSentiment Distribution:\")\n",
    "        print(self.news_data['sentiment_category'].value_counts())\n",
    "        \n",
    "        # Show sample results\n",
    "        print(\"\\nSample Sentiment Analysis Results:\")\n",
    "        sample_df = self.news_data[['headline', 'textblob_polarity', 'vader_compound', 'sentiment_category']].head()\n",
    "        for _, row in sample_df.iterrows():\n",
    "            print(f\"Headline: {row['headline'][:60]}...\")\n",
    "            print(f\"  TextBlob: {row['textblob_polarity']:.3f}, VADER: {row['vader_compound']:.3f}, Category: {row['sentiment_category']}\")\n",
    "            print()\n",
    "    \n",
    "    def aggregate_daily_sentiment(self):\n",
    "        \"\"\"Aggregate sentiment scores by date and stock\"\"\"\n",
    "        print(\"Aggregating Daily Sentiment Scores...\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Check if news data exists and has sentiment analysis completed\n",
    "        if self.news_data is None:\n",
    "            print(\"✗ Error: No news data loaded. Please load news data first.\")\n",
    "            return None\n",
    "\n",
    "        # Debug: Print column names and first few rows\n",
    "        print(\"Debug Info:\")\n",
    "        print(f\"Available columns: {list(self.news_data.columns)}\")\n",
    "        print(f\"Data shape: {self.news_data.shape}\")\n",
    "        print(\"\\nFirst few rows of date column:\")\n",
    "        if 'date' in self.news_data.columns:\n",
    "            print(self.news_data['date'].head())\n",
    "            print(f\"Date column dtype: {self.news_data['date'].dtype}\")\n",
    "            print(f\"Sample date values: {self.news_data['date'].iloc[:5].tolist()}\")\n",
    "\n",
    "        # Check if Date column exists, if not try to create it\n",
    "        if 'Date' not in self.news_data.columns:\n",
    "            print(\"⚠️  'Date' column not found. Attempting to create it...\")\n",
    "\n",
    "            if 'date' in self.news_data.columns:\n",
    "                # First, let's examine the date format\n",
    "                print(\"Analyzing date format...\")\n",
    "                sample_dates = self.news_data['date'].dropna().head()\n",
    "                print(f\"Sample dates: {sample_dates.tolist()}\")\n",
    "\n",
    "                try:\n",
    "                    # Try different approaches to convert dates\n",
    "                    print(\"Attempting to convert dates...\")\n",
    "\n",
    "                    # Method 1: Direct conversion\n",
    "                    try:\n",
    "                        self.news_data['Date'] = pd.to_datetime(self.news_data['date'], errors='coerce')\n",
    "                        print(\"✓ Method 1: Direct conversion successful\")\n",
    "                    except:\n",
    "                        print(\"✗ Method 1: Direct conversion failed\")\n",
    "\n",
    "                        # Method 2: Try with specific format inference\n",
    "                        try:\n",
    "                            self.news_data['Date'] = pd.to_datetime(self.news_data['date'], infer_datetime_format=True, errors='coerce')\n",
    "                            print(\"✓ Method 2: Format inference successful\")\n",
    "                        except:\n",
    "                            print(\"✗ Method 2: Format inference failed\")\n",
    "\n",
    "                            # Method 3: Try common date formats\n",
    "                            date_formats = ['%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y', '%Y/%m/%d', '%m-%d-%Y', '%d-%m-%Y']\n",
    "                            success = False\n",
    "\n",
    "                            for fmt in date_formats:\n",
    "                                try:\n",
    "                                    self.news_data['Date'] = pd.to_datetime(self.news_data['date'], format=fmt, errors='coerce')\n",
    "                                    print(f\"✓ Method 3: Format {fmt} successful\")\n",
    "                                    success = True\n",
    "                                    break\n",
    "                                except:\n",
    "                                    continue\n",
    "                                \n",
    "                            if not success:\n",
    "                                print(\"✗ All conversion methods failed\")\n",
    "                                return None\n",
    "\n",
    "                    # Check for NaT values after conversion\n",
    "                    nat_count = self.news_data['Date'].isna().sum()\n",
    "                    if nat_count > 0:\n",
    "                        print(f\"⚠️  Warning: {nat_count} dates could not be parsed and were set to NaT\")\n",
    "                        print(\"Dropping rows with invalid dates...\")\n",
    "                        self.news_data = self.news_data.dropna(subset=['Date'])\n",
    "                        print(f\"✓ Remaining rows after dropping invalid dates: {len(self.news_data)}\")\n",
    "\n",
    "                    print(\"✓ Date column created successfully\")\n",
    "                    print(f\"Date range: {self.news_data['Date'].min()} to {self.news_data['Date'].max()}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error converting dates: {str(e)}\")\n",
    "                    print(\"Please check your date format. Expected formats include:\")\n",
    "                    print(\"- YYYY-MM-DD (e.g., 2023-01-15)\")\n",
    "                    print(\"- MM/DD/YYYY (e.g., 01/15/2023)\")\n",
    "                    print(\"- DD/MM/YYYY (e.g., 15/01/2023)\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"✗ Error: No 'date' or 'Date' column found in news data\")\n",
    "                print(f\"Available columns: {list(self.news_data.columns)}\")\n",
    "                return None\n",
    "\n",
    "        # Verify Date column is datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.news_data['Date']):\n",
    "            print(\"⚠️  Converting 'Date' column to datetime...\")\n",
    "            try:\n",
    "                self.news_data['Date'] = pd.to_datetime(self.news_data['Date'], errors='coerce')\n",
    "                print(\"✓ 'Date' column converted to datetime\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error converting Date column: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "        # Check if sentiment analysis has been performed\n",
    "        required_columns = ['textblob_polarity', 'vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral']\n",
    "        missing_columns = [col for col in required_columns if col not in self.news_data.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"✗ Error: Sentiment analysis not completed. Missing columns: {missing_columns}\")\n",
    "            print(\"Please run perform_sentiment_analysis() first.\")\n",
    "            return None\n",
    "\n",
    "        # Check if stock column exists\n",
    "        if 'stock' not in self.news_data.columns:\n",
    "            print(\"✗ Error: 'stock' column not found in news data\")\n",
    "            print(f\"Available columns: {list(self.news_data.columns)}\")\n",
    "            return None\n",
    "\n",
    "        print(f\"✓ All required columns found. Proceeding with aggregation...\")\n",
    "        print(f\"Date range: {self.news_data['Date'].min()} to {self.news_data['Date'].max()}\")\n",
    "        print(f\"Unique stocks: {self.news_data['stock'].unique()}\")\n",
    "\n",
    "        try:\n",
    "            # Group by date and stock, calculate daily averages\n",
    "            daily_sentiment = self.news_data.groupby(['Date', 'stock']).agg({\n",
    "                'textblob_polarity': ['mean', 'std', 'count'],\n",
    "                'vader_compound': ['mean', 'std'],\n",
    "                'vader_positive': 'mean',\n",
    "                'vader_negative': 'mean',\n",
    "                'vader_neutral': 'mean'\n",
    "            }).reset_index()\n",
    "\n",
    "            # Flatten column names\n",
    "            daily_sentiment.columns = [\n",
    "                'Date', 'stock', 'avg_textblob_polarity', 'std_textblob_polarity', 'news_count',\n",
    "                'avg_vader_compound', 'std_vader_compound', 'avg_vader_positive', \n",
    "                'avg_vader_negative', 'avg_vader_neutral'\n",
    "            ]\n",
    "\n",
    "            # Fill NaN standard deviations with 0 (when only one news item per day)\n",
    "            daily_sentiment['std_textblob_polarity'] = daily_sentiment['std_textblob_polarity'].fillna(0)\n",
    "            daily_sentiment['std_vader_compound'] = daily_sentiment['std_vader_compound'].fillna(0)\n",
    "\n",
    "            self.daily_sentiment = daily_sentiment\n",
    "\n",
    "            print(f\"✓ Daily sentiment aggregated for {len(daily_sentiment)} stock-date combinations\")\n",
    "            print(f\"  Date range: {daily_sentiment['Date'].min().strftime('%Y-%m-%d')} to {daily_sentiment['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "            return daily_sentiment\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error during aggregation: {str(e)}\")\n",
    "            print(\"Debugging info:\")\n",
    "            print(f\"news_data shape: {self.news_data.shape}\")\n",
    "            print(f\"news_data columns: {list(self.news_data.columns)}\")\n",
    "            print(f\"Date column dtype: {self.news_data['Date'].dtype}\")\n",
    "            print(f\"Stock column dtype: {self.news_data['stock'].dtype}\")\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def merge_data_for_correlation(self):\n",
    "        \"\"\"Merge stock and sentiment data for correlation analysis\"\"\"\n",
    "        print(\"\\nMerging Stock and Sentiment Data...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Map stock symbols\n",
    "        stock_symbol_map = {\n",
    "            'AAPL': 'AAPL', \n",
    "            'AMZN': 'AMZN', \n",
    "            'GOOG': 'GOOG', \n",
    "            'MET': 'META', \n",
    "            'MSF': 'MSFT', \n",
    "            'NVDA': 'NVDA', \n",
    "            'TSLA': 'TSLA', \n",
    "        }\n",
    "        \n",
    "        merged_results = {}\n",
    "        \n",
    "        for news_symbol, stock_symbol in stock_symbol_map.items():\n",
    "            if stock_symbol in self.stock_data:\n",
    "                # Filter sentiment data for this stock\n",
    "                stock_sentiment = self.daily_sentiment[self.daily_sentiment['stock'] == news_symbol].copy()\n",
    "                \n",
    "                # Get stock data\n",
    "                stock_df = self.stock_data[stock_symbol].copy()\n",
    "                \n",
    "                # Merge on date\n",
    "                merged = pd.merge(\n",
    "                    stock_df[['Date', 'Close', 'Daily_Return', 'Volume', 'Volatility']], \n",
    "                    stock_sentiment, \n",
    "                    on='Date', \n",
    "                    how='inner'\n",
    "                )\n",
    "                \n",
    "                if len(merged) > 0:\n",
    "                    merged_results[stock_symbol] = merged\n",
    "                    print(f\" {stock_symbol}: {len(merged)} matching records\")\n",
    "                else:\n",
    "                    print(f\" {stock_symbol}: No matching dates found\")\n",
    "            else:\n",
    "                print(f\" Stock symbol {stock_symbol} not found in stock data\")\n",
    "        \n",
    "        self.merged_data = merged_results\n",
    "        return merged_results\n",
    "    \n",
    "    def calculate_correlations(self):\n",
    "        \"\"\"Calculate correlations between sentiment and stock returns\"\"\"\n",
    "        print(\"\\nCalculating Correlations...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        correlation_results = {}\n",
    "        \n",
    "        for symbol, data in self.merged_data.items():\n",
    "            print(f\"\\n{symbol} Analysis:\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            # Pearson correlations\n",
    "            metrics = ['Daily_Return', 'Volume', 'Volatility']\n",
    "            sentiment_metrics = ['avg_textblob_polarity', 'avg_vader_compound']\n",
    "            \n",
    "            for metric in metrics:\n",
    "                for sentiment in sentiment_metrics:\n",
    "                    try:\n",
    "                        corr_coef, p_value = pearsonr(data[sentiment], data[metric])\n",
    "                        results[f'{sentiment}_vs_{metric}'] = {\n",
    "                            'correlation': corr_coef,\n",
    "                            'p_value': p_value,\n",
    "                            'significant': p_value < 0.05\n",
    "                        }\n",
    "                        \n",
    "                        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "                        print(f\"{sentiment} vs {metric}: r={corr_coef:.4f}, p={p_value:.4f} {significance}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating correlation for {sentiment} vs {metric}: {str(e)}\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            results['summary'] = {\n",
    "                'total_observations': len(data),\n",
    "                'date_range': f\"{data['Date'].min().strftime('%Y-%m-%d')} to {data['Date'].max().strftime('%Y-%m-%d')}\",\n",
    "                'avg_daily_return': data['Daily_Return'].mean(),\n",
    "                'std_daily_return': data['Daily_Return'].std(),\n",
    "                'avg_sentiment_textblob': data['avg_textblob_polarity'].mean(),\n",
    "                'avg_sentiment_vader': data['avg_vader_compound'].mean()\n",
    "            }\n",
    "            \n",
    "            correlation_results[symbol] = results\n",
    "        \n",
    "        self.correlation_results = correlation_results\n",
    "        return correlation_results\n",
    "    \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"Create visualizations for the analysis\"\"\"\n",
    "        print(\"\\nCreating Visualizations...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        for symbol, data in self.merged_data.items():\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            fig.suptitle(f'{symbol}: News Sentiment vs Stock Movement Analysis', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # 1. Time series plot\n",
    "            ax1 = axes[0, 0]\n",
    "            ax1_twin = ax1.twinx()\n",
    "            \n",
    "            ax1.plot(data['Date'], data['Daily_Return'], color='blue', alpha=0.7, label='Daily Return (%)')\n",
    "            ax1_twin.plot(data['Date'], data['avg_textblob_polarity'], color='red', alpha=0.7, label='Sentiment Score')\n",
    "            \n",
    "            ax1.set_ylabel('Daily Return (%)', color='blue')\n",
    "            ax1_twin.set_ylabel('Sentiment Score', color='red')\n",
    "            ax1.set_title('Daily Returns vs Sentiment Over Time')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 2. Scatter plot - TextBlob\n",
    "            ax2 = axes[0, 1]\n",
    "            scatter = ax2.scatter(data['avg_textblob_polarity'], data['Daily_Return'], \n",
    "                                alpha=0.6, c=data['news_count'], cmap='viridis')\n",
    "            ax2.set_xlabel('Average TextBlob Sentiment')\n",
    "            ax2.set_ylabel('Daily Return (%)')\n",
    "            ax2.set_title('Sentiment vs Daily Returns (TextBlob)')\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(data['avg_textblob_polarity'], data['Daily_Return'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax2.plot(data['avg_textblob_polarity'], p(data['avg_textblob_polarity']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            plt.colorbar(scatter, ax=ax2, label='News Count')\n",
    "            \n",
    "            # 3. Scatter plot - VADER\n",
    "            ax3 = axes[1, 0]\n",
    "            scatter2 = ax3.scatter(data['avg_vader_compound'], data['Daily_Return'], \n",
    "                                 alpha=0.6, c=data['news_count'], cmap='viridis')\n",
    "            ax3.set_xlabel('Average VADER Sentiment')\n",
    "            ax3.set_ylabel('Daily Return (%)')\n",
    "            ax3.set_title('Sentiment vs Daily Returns (VADER)')\n",
    "            \n",
    "            # Add trend line\n",
    "            z2 = np.polyfit(data['avg_vader_compound'], data['Daily_Return'], 1)\n",
    "            p2 = np.poly1d(z2)\n",
    "            ax3.plot(data['avg_vader_compound'], p2(data['avg_vader_compound']), \"r--\", alpha=0.8)\n",
    "            \n",
    "            plt.colorbar(scatter2, ax=ax3, label='News Count')\n",
    "            \n",
    "            # 4. Sentiment distribution\n",
    "            ax4 = axes[1, 1]\n",
    "            sentiment_data = self.news_data[self.news_data['stock'] == data['stock'].iloc[0]]\n",
    "            ax4.hist([sentiment_data['textblob_polarity'], sentiment_data['vader_compound']], \n",
    "                    bins=20, alpha=0.7, label=['TextBlob', 'VADER'])\n",
    "            ax4.set_xlabel('Sentiment Score')\n",
    "            ax4.set_ylabel('Frequency')\n",
    "            ax4.set_title('Sentiment Score Distribution')\n",
    "            ax4.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Create correlation heatmap\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "            \n",
    "            # Prepare correlation matrix\n",
    "            corr_data = data[['Daily_Return', 'avg_textblob_polarity', 'avg_vader_compound', \n",
    "                            'Volume', 'news_count']].corr()\n",
    "            \n",
    "            # Create heatmap\n",
    "            sns.heatmap(corr_data, annot=True, cmap='RdBu_r', center=0, \n",
    "                       square=True, ax=ax, cbar_kws={'label': 'Correlation Coefficient'})\n",
    "            ax.set_title(f'{symbol}: Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"NEWS SENTIMENT AND STOCK MOVEMENT CORRELATION ANALYSIS REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nAnalysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Stocks Analyzed: {list(self.correlation_results.keys())}\")\n",
    "        \n",
    "        for symbol, results in self.correlation_results.items():\n",
    "            print(f\"\\n{symbol} - DETAILED ANALYSIS\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            summary = results['summary']\n",
    "            print(f\"Period: {summary['date_range']}\")\n",
    "            print(f\"Total Observations: {summary['total_observations']}\")\n",
    "            print(f\"Average Daily Return: {summary['avg_daily_return']:.4f}%\")\n",
    "            print(f\"Daily Return Volatility: {summary['std_daily_return']:.4f}%\")\n",
    "            print(f\"Average TextBlob Sentiment: {summary['avg_sentiment_textblob']:.4f}\")\n",
    "            print(f\"Average VADER Sentiment: {summary['avg_sentiment_vader']:.4f}\")\n",
    "            \n",
    "            print(f\"\\nCORRELATION ANALYSIS:\")\n",
    "            print(\"Key Findings:\")\n",
    "            \n",
    "            # Find strongest correlations\n",
    "            strongest_corr = 0\n",
    "            strongest_pair = \"\"\n",
    "            \n",
    "            for key, value in results.items():\n",
    "                if key != 'summary' and abs(value['correlation']) > abs(strongest_corr):\n",
    "                    strongest_corr = value['correlation']\n",
    "                    strongest_pair = key\n",
    "            \n",
    "            print(f\"• Strongest correlation: {strongest_pair}\")\n",
    "            print(f\"  Correlation coefficient: {strongest_corr:.4f}\")\n",
    "        \n",
    "        print(f\"\\nKEY INSIGHTS:\")\n",
    "        print(\"• This analysis examines the relationship between news sentiment and stock price movements\")\n",
    "        print(\"• Correlation coefficients range from -1 to +1\")\n",
    "        print(\"• Values closer to +1 indicate positive correlation (positive news → positive returns)\")\n",
    "        print(\"• Values closer to -1 indicate negative correlation (positive news → negative returns)\")\n",
    "        print(\"• P-values < 0.05 indicate statistically significant relationships\")\n",
    "        \n",
    "        print(f\"\\nRECOMMendations FOR FURTHER ANALYSIS:\")\n",
    "        print(\"• Increase sample size with more news data\")\n",
    "        print(\"• Analyze lagged correlations (news impact on next-day returns)\")\n",
    "        print(\"• Consider market conditions and external factors\")\n",
    "        print(\"• Implement more sophisticated sentiment analysis models\")\n",
    "        print(\"• Analyze sector-specific patterns\")\n",
    "\n",
    "# Main execution function\n",
    "def run_analysis():\n",
    "    \"\"\"Main function to run the complete analysis\"\"\"\n",
    "    \n",
    "    analyzer = NewsStockCorrelationAnalyzer()\n",
    "    \n",
    "    stock_files = {\n",
    "        'AAPL': '../data/yfinance_data/AAPL_historical_data.csv',\n",
    "        'AMZN': '../data/yfinance_data/AMZN_historical_data.csv',\n",
    "        'GOOG': '../data/yfinance_data/GOOG_historical_data.csv',\n",
    "        'META': '../data/yfinance_data/META_historical_data.csv',\n",
    "        'MSFT': '../data/yfinance_data/MSFT_historical_data.csv',\n",
    "        'NVDA': '../data/yfinance_data/NVDA_historical_data.csv',\n",
    "        'TSLA': '../data/yfinance_data/TSLA_historical_data.csv'\n",
    "    }\n",
    "    \n",
    "    # Load data\n",
    "    analyzer.load_stock_data(stock_files)\n",
    "    analyzer.load_news_data('../data/raw_analyst_ratings.csv/raw_analyst_ratings.csv') \n",
    "    \n",
    "    # Perform analysis\n",
    "    analyzer.perform_sentiment_analysis()\n",
    "    analyzer.aggregate_daily_sentiment()\n",
    "    analyzer.merge_data_for_correlation()\n",
    "    analyzer.calculate_correlations()\n",
    "    \n",
    "    # Create visualizations\n",
    "    analyzer.create_visualizations()\n",
    "    \n",
    "    # Generate report\n",
    "    analyzer.generate_report()\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Task 3: News Sentiment and Stock Movement Correlation Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Run the complete analysis\n",
    "    analyzer = run_analysis()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
