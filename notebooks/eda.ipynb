{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f395cae",
   "metadata": {},
   "source": [
    "# Financial News Dataset - Exploratory Data Analysis (EDA)\n",
    "# ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883771cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af64e7",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 1. DATA LOADING AND INITIAL EXPLORATION\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(file_path):\n",
    "    \"\"\"Load the dataset and perform initial exploration\"\"\"\n",
    "    print(\"ðŸ” LOADING AND EXPLORING DATA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_and_explore_data('../data/raw_analyst_ratings/raw_analyst_ratings.csv')\n",
    "print(f\"Dataset Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c628c",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 2. DATA PREPROCESSING\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and preprocess the data\"\"\"\n",
    "    print(\"\\nðŸ”§ DATA PREPROCESSING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')\n",
    "    \n",
    "    df_clean['year'] = df_clean['date'].dt.year\n",
    "    df_clean['month'] = df_clean['date'].dt.month\n",
    "    df_clean['day'] = df_clean['date'].dt.day\n",
    "    df_clean['hour'] = df_clean['date'].dt.hour\n",
    "    df_clean['day_of_week'] = df_clean['date'].dt.day_name()\n",
    "    df_clean['is_weekend'] = df_clean['date'].dt.weekday >= 5\n",
    "    \n",
    "    df_clean['headline_length'] = df_clean['headline'].str.len()\n",
    "    \n",
    "    df_clean['domain'] = df_clean['url'].str.extract(r'https?://(?:www\\.)?([^/]+)')\n",
    "    \n",
    "    df_clean['is_email_publisher'] = df_clean['publisher'].str.contains('@', na=False)\n",
    "    df_clean['publisher_domain'] = df_clean['publisher'].str.extract(r'@([^.]+\\.[^.]+)').fillna('')\n",
    "    \n",
    "    print(\"Preprocessing completed!\")\n",
    "    print(f\"Date range: {df_clean['date'].min()} to {df_clean['date'].max()}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_processed = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ed776",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 3. DESCRIPTIVE STATISTICS\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics(df):\n",
    "    \"\"\"Generate comprehensive descriptive statistics\"\"\"\n",
    "    print(\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_articles = len(df)\n",
    "    unique_publishers = df['publisher'].nunique()\n",
    "    unique_stocks = df['stock'].nunique()\n",
    "    unique_domains = df['domain'].nunique()\n",
    "    \n",
    "    # Headline length statistics\n",
    "    headline_stats = df['headline_length'].describe()\n",
    "    \n",
    "    print(f\"ðŸ“Š BASIC METRICS:\")\n",
    "    print(f\"   Total Articles: {total_articles:,}\")\n",
    "    print(f\"   Unique Publishers: {unique_publishers:,}\")\n",
    "    print(f\"   Unique Stocks: {unique_stocks:,}\")\n",
    "    print(f\"   Unique Domains: {unique_domains:,}\")\n",
    "    print(f\"   Email Publishers: {df['is_email_publisher'].sum():,}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ HEADLINE LENGTH STATISTICS:\")\n",
    "    for stat, value in headline_stats.items():\n",
    "        print(f\"   {stat.title()}: {value:.1f} characters\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    axes[0, 0].hist(df['headline_length'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribution of Headline Lengths')\n",
    "    axes[0, 0].set_xlabel('Headline Length (characters)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(headline_stats['mean'], color='red', linestyle='--', label=f'Mean: {headline_stats[\"mean\"]:.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Top publishers\n",
    "    top_publishers = df['publisher'].value_counts().head(10)\n",
    "    axes[0, 1].barh(range(len(top_publishers)), top_publishers.values)\n",
    "    axes[0, 1].set_yticks(range(len(top_publishers)))\n",
    "    axes[0, 1].set_yticklabels(top_publishers.index)\n",
    "    axes[0, 1].set_title('Top 10 Publishers by Article Count')\n",
    "    axes[0, 1].set_xlabel('Number of Articles')\n",
    "    \n",
    "    # Stock distribution\n",
    "    stock_counts = df['stock'].value_counts().head(10)\n",
    "    axes[1, 0].pie(stock_counts.values, labels=stock_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1, 0].set_title('Distribution of Articles by Stock')\n",
    "    \n",
    "    # Email vs Non-email publishers\n",
    "    email_counts = df['is_email_publisher'].value_counts()\n",
    "    axes[1, 1].bar(['Non-Email', 'Email'], email_counts.values, color=['lightcoral', 'lightblue'])\n",
    "    axes[1, 1].set_title('Email vs Non-Email Publishers')\n",
    "    axes[1, 1].set_ylabel('Number of Articles')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_articles': total_articles,\n",
    "        'unique_publishers': unique_publishers,\n",
    "        'headline_stats': headline_stats,\n",
    "        'top_publishers': top_publishers\n",
    "    }\n",
    "\n",
    "desc_stats = descriptive_statistics(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777fd6e",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 4. TIME SERIES ANALYSIS\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_analysis(df):\n",
    "    \"\"\"Perform comprehensive time series analysis\"\"\"\n",
    "    print(\"\\nâ° TIME SERIES ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    daily_counts = df.groupby(df['date'].dt.date).size().reset_index()\n",
    "    daily_counts.columns = ['date', 'count']\n",
    "    daily_counts['date'] = pd.to_datetime(daily_counts['date'])\n",
    "    \n",
    "    hourly_counts = df.groupby('hour').size()\n",
    "    \n",
    "    # Day of week distribution\n",
    "    dow_counts = df.groupby('day_of_week').size()\n",
    "    dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    dow_counts = dow_counts.reindex(dow_order, fill_value=0)\n",
    "    \n",
    "    # Monthly trends\n",
    "    monthly_counts = df.groupby([df['date'].dt.year, df['date'].dt.month]).size().reset_index()\n",
    "    monthly_counts['date'] = pd.to_datetime(monthly_counts[['year', 'month']].assign(day=1))\n",
    "    \n",
    "    print(f\"ðŸ“… TIME SERIES INSIGHTS:\")\n",
    "    print(f\"   Date Range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Most Active Day: {dow_counts.idxmax()} ({dow_counts.max()} articles)\")\n",
    "    print(f\"   Most Active Hour: {hourly_counts.idxmax()}:00 ({hourly_counts.max()} articles)\")\n",
    "    print(f\"   Weekend Articles: {df['is_weekend'].sum()} ({df['is_weekend'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Daily publication frequency\n",
    "    axes[0, 0].plot(daily_counts['date'], daily_counts['count'], marker='o', linewidth=2, markersize=4)\n",
    "    axes[0, 0].set_title('Daily Publication Frequency')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Number of Articles')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hourly distribution\n",
    "    axes[0, 1].bar(hourly_counts.index, hourly_counts.values, color='lightgreen', alpha=0.7)\n",
    "    axes[0, 1].set_title('Publication Distribution by Hour of Day')\n",
    "    axes[0, 1].set_xlabel('Hour of Day')\n",
    "    axes[0, 1].set_ylabel('Number of Articles')\n",
    "    axes[0, 1].set_xticks(range(0, 24, 2))\n",
    "    \n",
    "    # Day of week distribution\n",
    "    axes[1, 0].bar(range(len(dow_counts)), dow_counts.values, color='orange', alpha=0.7)\n",
    "    axes[1, 0].set_title('Publication Distribution by Day of Week')\n",
    "    axes[1, 0].set_xlabel('Day of Week')\n",
    "    axes[1, 0].set_ylabel('Number of Articles')\n",
    "    axes[1, 0].set_xticks(range(len(dow_counts)))\n",
    "    axes[1, 0].set_xticklabels([day[:3] for day in dow_order], rotation=45)\n",
    "    \n",
    "    # Monthly trends (if applicable)\n",
    "    if len(monthly_counts) > 1:\n",
    "        axes[1, 1].plot(monthly_counts['date'], monthly_counts[0], marker='s', linewidth=2, markersize=6, color='purple')\n",
    "        axes[1, 1].set_title('Monthly Publication Trends')\n",
    "        axes[1, 1].set_xlabel('Month')\n",
    "        axes[1, 1].set_ylabel('Number of Articles')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Insufficient data\\nfor monthly trends', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "        axes[1, 1].set_title('Monthly Publication Trends')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'daily_counts': daily_counts,\n",
    "        'hourly_counts': hourly_counts,\n",
    "        'dow_counts': dow_counts,\n",
    "        'peak_hour': hourly_counts.idxmax(),\n",
    "        'peak_day': dow_counts.idxmax()\n",
    "    }\n",
    "\n",
    "time_analysis = time_series_analysis(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b946c",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 5. TEXT ANALYSIS & TOPIC MODELING\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1baca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis(df):\n",
    "    \"\"\"Perform comprehensive text analysis on headlines\"\"\"\n",
    "    print(\"\\nðŸ“ TEXT ANALYSIS & TOPIC MODELING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Combine all headlines\n",
    "    all_text = ' '.join(df['headline'].astype(str))\n",
    "    \n",
    "    # Common financial keywords to look for\n",
    "    financial_keywords = [\n",
    "        'price target', 'target price', 'maintains', 'raises', 'lowers', 'upgrades', 'downgrades',\n",
    "        'earnings', 'revenue', 'eps', 'quarterly', 'annual', 'q1', 'q2', 'q3', 'q4',\n",
    "        'fda approval', 'fda', 'approval', 'merger', 'acquisition', 'dividend', 'stock split',\n",
    "        'buyback', 'shares', 'trading', 'higher', 'lower', 'up', 'down', 'bull', 'bear',\n",
    "        'market', 'stock', 'analyst', 'rating', 'recommendation', 'buy', 'sell', 'hold',\n",
    "        'neutral', 'overweight', 'underweight', 'outperform', 'underperform'\n",
    "    ]\n",
    "    \n",
    "    keyword_counts = {}\n",
    "    for keyword in financial_keywords:\n",
    "        count = all_text.lower().count(keyword.lower())\n",
    "        if count > 0:\n",
    "            keyword_counts[keyword] = count\n",
    "    \n",
    "    stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'])\n",
    "    \n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', all_text.lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    word_counts = Counter(filtered_words)\n",
    "    \n",
    "    print(f\"ðŸ”¤ TEXT ANALYSIS RESULTS:\")\n",
    "    print(f\"   Total Words: {len(words):,}\")\n",
    "    print(f\"   Unique Words: {len(set(words)):,}\")\n",
    "    print(f\"   Financial Keywords Found: {len(keyword_counts)}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ TOP FINANCIAL KEYWORDS:\")\n",
    "    sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for keyword, count in sorted_keywords:\n",
    "        print(f\"   '{keyword}': {count} occurrences\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Top words\n",
    "    top_words = dict(word_counts.most_common(15))\n",
    "    axes[0, 0].barh(list(top_words.keys()), list(top_words.values()))\n",
    "    axes[0, 0].set_title('Top 15 Most Frequent Words')\n",
    "    axes[0, 0].set_xlabel('Frequency')\n",
    "    \n",
    "    # Financial keywords\n",
    "    if sorted_keywords:\n",
    "        kw_names, kw_counts = zip(*sorted_keywords[:10])\n",
    "        axes[0, 1].bar(range(len(kw_names)), kw_counts, color='lightcoral')\n",
    "        axes[0, 1].set_title('Top Financial Keywords')\n",
    "        axes[0, 1].set_xlabel('Keywords')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_xticks(range(len(kw_names)))\n",
    "        axes[0, 1].set_xticklabels(kw_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Word length distribution\n",
    "    word_lengths = [len(word) for word in filtered_words]\n",
    "    axes[1, 0].hist(word_lengths, bins=15, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[1, 0].set_title('Distribution of Word Lengths')\n",
    "    axes[1, 0].set_xlabel('Word Length (characters)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Create word cloud\n",
    "    if len(filtered_words) > 10:\n",
    "        try:\n",
    "            wordcloud = WordCloud(width=400, height=300, background_color='white', \n",
    "                                max_words=50, colormap='viridis').generate(' '.join(filtered_words))\n",
    "            axes[1, 1].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[1, 1].axis('off')\n",
    "            axes[1, 1].set_title('Word Cloud of Headlines')\n",
    "        except:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Word Cloud\\nNot Available', \n",
    "                           ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'word_counts': word_counts,\n",
    "        'keyword_counts': keyword_counts,\n",
    "        'top_words': top_words,\n",
    "        'total_words': len(words),\n",
    "        'unique_words': len(set(words))\n",
    "    }\n",
    "\n",
    "text_results = text_analysis(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc0647",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 6. PUBLISHER ANALYSIS\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def publisher_analysis(df):\n",
    "    \"\"\"Analyze publisher patterns and characteristics\"\"\"\n",
    "    print(\"\\nðŸ‘¥ PUBLISHER ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    publisher_stats = df['publisher'].value_counts()\n",
    "    email_publishers = df[df['is_email_publisher'] == True]['publisher'].value_counts()\n",
    "    domain_stats = df[df['publisher_domain'] != '']['publisher_domain'].value_counts()\n",
    "    \n",
    "    publisher_activity = df.groupby('publisher').agg({\n",
    "        'headline_length': ['mean', 'std'],\n",
    "        'date': ['min', 'max', 'count'],\n",
    "        'stock': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    publisher_activity.columns = ['avg_headline_length', 'std_headline_length', \n",
    "                                'first_article', 'last_article', 'total_articles', 'unique_stocks']\n",
    "    \n",
    "    print(f\"ðŸ“Š PUBLISHER STATISTICS:\")\n",
    "    print(f\"   Total Publishers: {len(publisher_stats)}\")\n",
    "    print(f\"   Email-based Publishers: {len(email_publishers)}\")\n",
    "    print(f\"   Most Active Publisher: {publisher_stats.index[0]} ({publisher_stats.iloc[0]} articles)\")\n",
    "    \n",
    "    if len(domain_stats) > 0:\n",
    "        print(f\"   Most Common Domain: {domain_stats.index[0]} ({domain_stats.iloc[0]} articles)\")\n",
    "    \n",
    "    print(f\"\\nðŸ† TOP 5 PUBLISHERS:\")\n",
    "    for i, (publisher, count) in enumerate(publisher_stats.head().items(), 1):\n",
    "        print(f\"   {i}. {publisher}: {count} articles\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    top_publishers = publisher_stats.head(10)\n",
    "    axes[0, 0].barh(range(len(top_publishers)), top_publishers.values)\n",
    "    axes[0, 0].set_yticks(range(len(top_publishers)))\n",
    "    axes[0, 0].set_yticklabels(top_publishers.index)\n",
    "    axes[0, 0].set_title('Top 10 Publishers by Article Count')\n",
    "    axes[0, 0].set_xlabel('Number of Articles')\n",
    "    \n",
    "    email_dist = df['is_email_publisher'].value_counts()\n",
    "    axes[0, 1].pie(email_dist.values, labels=['Non-Email', 'Email'], autopct='%1.1f%%', \n",
    "                   colors=['lightblue', 'lightcoral'], startangle=90)\n",
    "    axes[0, 1].set_title('Email vs Non-Email Publishers')\n",
    "    \n",
    "    if len(publisher_activity) > 1:\n",
    "        top_pub_headlines = publisher_activity.head(8)['avg_headline_length']\n",
    "        axes[1, 0].bar(range(len(top_pub_headlines)), top_pub_headlines.values, color='lightgreen')\n",
    "        axes[1, 0].set_title('Average Headline Length by Top Publishers')\n",
    "        axes[1, 0].set_xlabel('Publishers')\n",
    "        axes[1, 0].set_ylabel('Average Headline Length')\n",
    "        axes[1, 0].set_xticks(range(len(top_pub_headlines)))\n",
    "        axes[1, 0].set_xticklabels(top_pub_headlines.index, rotation=45, ha='right')\n",
    "    \n",
    "    if len(domain_stats) > 0:\n",
    "        axes[1, 1].bar(range(len(domain_stats)), domain_stats.values, color='orange')\n",
    "        axes[1, 1].set_title('Articles by Email Domain')\n",
    "        axes[1, 1].set_xlabel('Email Domains')\n",
    "        axes[1, 1].set_ylabel('Number of Articles')\n",
    "        axes[1, 1].set_xticks(range(len(domain_stats)))\n",
    "        axes[1, 1].set_xticklabels(domain_stats.index, rotation=45, ha='right')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No Email Domains\\nFound', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'publisher_stats': publisher_stats,\n",
    "        'email_publishers': email_publishers,\n",
    "        'domain_stats': domain_stats,\n",
    "        'publisher_activity': publisher_activity\n",
    "    }\n",
    "\n",
    "publisher_results = publisher_analysis(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523c9e4",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### 7. ADVANCED ANALYSIS & INSIGHTS\n",
    "### ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_insights(df):\n",
    "    \"\"\"Generate advanced insights and correlations\"\"\"\n",
    "    print(\"\\nðŸŽ¯ ADVANCED INSIGHTS & CORRELATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Correlation analysis\n",
    "    numeric_cols = ['headline_length', 'hour', 'day', 'month']\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    publisher_stock_diversity = df.groupby('publisher')['stock'].nunique().sort_values(ascending=False)\n",
    "    \n",
    "    # Time-based patterns\n",
    "    hour_publisher = pd.crosstab(df['hour'], df['publisher'])\n",
    "    \n",
    "    # Peak activity analysis\n",
    "    peak_hours = df['hour'].value_counts().head(3)\n",
    "    peak_days = df['day_of_week'].value_counts().head(3)\n",
    "    \n",
    "    print(f\"ðŸ” ADVANCED INSIGHTS:\")\n",
    "    print(f\"   Most Diverse Publisher: {publisher_stock_diversity.index[0]} ({publisher_stock_diversity.iloc[0]} stocks)\")\n",
    "    print(f\"   Peak Publishing Hours: {', '.join([f'{h}:00' for h in peak_hours.index])}\")\n",
    "    print(f\"   Peak Publishing Days: {', '.join(peak_days.index)}\")\n",
    "    \n",
    "    hourly_headline_length = df.groupby('hour')['headline_length'].mean()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š HEADLINE PATTERNS:\")\n",
    "    longest_avg_hour = hourly_headline_length.idxmax()\n",
    "    shortest_avg_hour = hourly_headline_length.idxmin()\n",
    "    print(f\"   Longest headlines published at: {longest_avg_hour}:00 (avg: {hourly_headline_length[longest_avg_hour]:.1f} chars)\")\n",
    "    print(f\"   Shortest headlines published at: {shortest_avg_hour}:00 (avg: {hourly_headline_length[shortest_avg_hour]:.1f} chars)\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Correlation Matrix of Numeric Variables')\n",
    "    \n",
    "    # Publisher stock diversity\n",
    "    top_diverse = publisher_stock_diversity.head(8)\n",
    "    axes[0, 1].bar(range(len(top_diverse)), top_diverse.values, color='lightpink')\n",
    "    axes[0, 1].set_title('Publisher Stock Diversity')\n",
    "    axes[0, 1].set_xlabel('Publishers')\n",
    "    axes[0, 1].set_ylabel('Number of Unique Stocks')\n",
    "    axes[0, 1].set_xticks(range(len(top_diverse)))\n",
    "    axes[0, 1].set_xticklabels(top_diverse.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Hourly headline length trends\n",
    "    axes[1, 0].plot(hourly_headline_length.index, hourly_headline_length.values, \n",
    "                   marker='o', linewidth=2, markersize=6, color='green')\n",
    "    axes[1, 0].set_title('Average Headline Length by Hour')\n",
    "    axes[1, 0].set_xlabel('Hour of Day')\n",
    "    axes[1, 0].set_ylabel('Average Headline Length')\n",
    "    axes[1, 0].set_xticks(range(0, 24, 2))\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Publication frequency heatmap (hour vs day of week)\n",
    "    if len(df) > 10:  # Only if we have sufficient data\n",
    "        hour_dow = pd.crosstab(df['hour'], df['day_of_week'])\n",
    "        \n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        hour_dow = hour_dow.reindex(columns=[day for day in day_order if day in hour_dow.columns])\n",
    "        \n",
    "        sns.heatmap(hour_dow, cmap='YlOrRd', ax=axes[1, 1], cbar_kws={'label': 'Number of Articles'})\n",
    "        axes[1, 1].set_title('Publication Heatmap: Hour vs Day of Week')\n",
    "        axes[1, 1].set_xlabel('Day of Week')\n",
    "        axes[1, 1].set_ylabel('Hour of Day')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Insufficient Data\\nfor Heatmap', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'correlation_matrix': correlation_matrix,\n",
    "        'publisher_diversity': publisher_stock_diversity,\n",
    "        'peak_hours': peak_hours,\n",
    "        'hourly_headline_length': hourly_headline_length\n",
    "    }\n",
    "\n",
    "advanced_results = advanced_insights(df_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
